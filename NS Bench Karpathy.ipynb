{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J_NxYcZUWqk_"
   },
   "source": [
    "# Neurónová sieť text po znakoch Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "id": "Hv8rrCTrtEbv"
   },
   "outputs": [],
   "source": [
    "# importy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "collapsed": true,
    "id": "4xWKAFegJ8Jc",
    "outputId": "bb34f0b8-a983-4b3c-cd38-b9e8ad730068"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU\n",
      "fyzické jadrá:  20\n",
      "logické jadrá  28\n"
     ]
    }
   ],
   "source": [
    "# === trénovanie modelu CPU ===\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "#nParametre CPU\n",
    "import psutil\n",
    "print(\"CPU\")\n",
    "print (\"fyzické jadrá: \",psutil.cpu_count(logical=False))\n",
    "print (\"logické jadrá \",psutil.cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === trénovanie modelu GPU ===\n",
    "from torch.cuda import is_available\n",
    "\n",
    "# Pokiaľ možno na GPU\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "  device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Počet znakov textu:  551955\n",
      " !&(),-./0124589:;?ABCDEFGHIJKLMNOPRSTUVWXYZabcdefghijklmnopqrstuvwxyz|ÁÇÔÚáäçéëíóôöúýČčĎďĺĽľŁňŕśŠšŤťŽž├┬┼\n",
      " 0124589ABCDEFGHIJKLMNOPRSTUVWXYZabcdefghijklmnopqrstuvwxyzÁÇÔÚáäçéëíóôöúýČčĎďĺĽľŁňŕśŠšŤťŽž\n",
      "Počet unikátnych znakov:  91\n"
     ]
    }
   ],
   "source": [
    "# Príprava textu\n",
    "# ==============\n",
    "\n",
    "#Načítanie knihy z lokálneho PC \n",
    "kniha = open('./Rivers of Babylon.txt', 'r', encoding=\"utf8\")\n",
    "text = kniha.read().replace('\\n', '')\n",
    "print(\"Počet znakov textu: \", len(text))\n",
    "# všetky unikátne znaky v texte\n",
    "znaky = sorted(list(set(text)))  # kvôli prehľadnosti utriedené\n",
    "print(''.join(znaky))\n",
    "\n",
    "#odstránenie neabecedných znakov\n",
    "neabecedne_znaky = string.punctuation # zoznam nealfanumerických znakov\n",
    "neabecedne_znaky += '├┬┼'   # ďalšie znaky ktoré vidím že sa mi tam vyskytujú\n",
    "for znak in neabecedne_znaky:\n",
    "   text = text.replace(znak, \"\")\n",
    "znaky = sorted(list(set(text)))\n",
    "unikatnych_znakov = len(znaky)   #vocab_size\n",
    "print(''.join(znaky))\n",
    "print(\"Počet unikátnych znakov: \",unikatnych_znakov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "id": "TstpF9Q6TBdv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([529722])\n",
      "Trénovacia množina: torch.Size([476749])\n",
      "Testovacia množina: torch.Size([52973])\n"
     ]
    }
   ],
   "source": [
    "# mapovanie znakov na indexy\n",
    "znaky_na_ix = { ch:i for i,ch in enumerate(znaky) }\n",
    "znaky_na_ix\n",
    "\n",
    "# indexy na znaky\n",
    "ix_na_znaky = { i:ch for i,ch in enumerate(znaky) }\n",
    "ix_na_znaky\n",
    "\n",
    "#kodovanie\n",
    "text_na_cisla = lambda t: [znaky_na_ix[c] for c in t]\n",
    "#dekodovanie\n",
    "cisla_na_text = lambda c: ''.join([ix_na_znaky[i] for i in c])\n",
    "\n",
    "# konverzia textu na tenzory\n",
    "tenzory_textu = torch.tensor(text_na_cisla(text), dtype=torch.long)\n",
    "print(tenzory_textu.shape)\n",
    "\n",
    "# rozdelenie dát na trénovaciu a testovaciu množinu\n",
    "# tu záleží na postupnosti dát (tenzorov zastupujúcich znaky)\n",
    "# takže to musí byť rozdelené sekvenčne nie náhodne\n",
    "n = int(0.9*len(tenzory_textu)) # 90%  textu od začiatku budú trénovacie dáta\n",
    "train_data = tenzory_textu[:n]\n",
    "test_data = tenzory_textu[n:]\n",
    "print(\"Trénovacia množina:\",train_data.shape)\n",
    "print(\"Testovacia množina:\",test_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# načítanie dávky vstupných a výstupných údajov začiatok dávky ix náhodne\n",
    "def nacitanie_davky(split):\n",
    "    temp_data = train_data if split == 'train' else test_data\n",
    "    ix = torch.randint(len(temp_data) - block_size, (batch_size,)) #náhodná poloha v texte\n",
    "    x = torch.stack([temp_data[i:i+block_size] for i in ix])      #vstupy\n",
    "    y = torch.stack([temp_data[i+1:i+block_size+1] for i in ix])  #výstupy\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "torch.manual_seed(1337)  #inicializácia generátora náhodných čísel\n",
    "batch_size = 4   # počet paralelne spracovávaných sekvencií 4\n",
    "block_size = 8   # maximálna dĺžka kontextu pre predikciu 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "XXbhZl821Jjs"
   },
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = nacitanie_davky(split)\n",
    "            logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# one head of self-attention\n",
    "class Head(nn.Module): \n",
    "    def __init__(self, head_size):\n",
    "        super().__init__()\n",
    "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
    "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input of size (batch, time-step, channels)\n",
    "        # output of size (batch, time-step, head size)\n",
    "        B,T,C = x.shape\n",
    "        k = self.key(x)   # (B,T,hs)\n",
    "        q = self.query(x) # (B,T,hs)\n",
    "        # compute attention scores (\"affinities\")\n",
    "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
    "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
    "        wei = self.dropout(wei)\n",
    "        # perform the weighted aggregation of the values\n",
    "        v = self.value(x) # (B,T,hs)\n",
    "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
    "        return out\n",
    "\n",
    "# multiple heads of self-attention in parallel\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, head_size):\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
    "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        out = self.dropout(self.proj(out))\n",
    "        return out\n",
    "    \n",
    "# a simple linear layer followed by a non-linearity\n",
    "class FeedFoward(nn.Module):\n",
    "    def __init__(self, n_embd):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_embd, 4 * n_embd),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(4 * n_embd, n_embd),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)    \n",
    "\n",
    "    # Transformer block: communication followed by computation\n",
    "class Block(nn.Module):\n",
    "    def __init__(self, n_embd, n_head):\n",
    "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
    "        super().__init__()\n",
    "        head_size = n_embd // n_head\n",
    "        self.sa = MultiHeadAttention(n_head, head_size)\n",
    "        self.ffwd = FeedFoward(n_embd)\n",
    "        self.ln1 = nn.LayerNorm(n_embd)\n",
    "        self.ln2 = nn.LayerNorm(n_embd)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "id": "eow4Po5QwP60"
   },
   "outputs": [],
   "source": [
    "# ===  Model neurónovej siete ====\n",
    "class GPTLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(unikatnych_znakov, n_embd)\n",
    "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
    "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
    "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
    "        self.lm_head = nn.Linear(n_embd, unikatnych_znakov)\n",
    "\n",
    "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "\n",
    "        # idx and targets are both (B,T) tensor of integers\n",
    "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
    "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
    "        x = tok_emb + pos_emb # (B,T,C)\n",
    "        x = self.blocks(x) # (B,T,C)\n",
    "        x = self.ln_f(x) # (B,T,C)\n",
    "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
    "\n",
    "        if targets is None:\n",
    "            loss = None\n",
    "        else:\n",
    "            B, T, C = logits.shape\n",
    "            logits = logits.view(B*T, C)\n",
    "            targets = targets.view(B*T)\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx, max_new_tokens):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop idx to the last block_size tokens\n",
    "            idx_cond = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self(idx_cond)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :] # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mASF8QWc4J1V",
    "outputId": "3b207a09-c899-4494-bac7-42f6be0677ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.808923 M parameters\n"
     ]
    }
   ],
   "source": [
    "# globálne parametre\n",
    "batch_size = 64         # počet paralelne spracovávaných sekvencií\n",
    "block_size = 256         # maximálna dĺžka kontextu pre predikciu (slovo ako blok znakov)\n",
    "max_iters = 5000        # počet iterácií\n",
    "eval_interval = 500\n",
    "learning_rate = 3e-4\n",
    "\n",
    "eval_iters = 200\n",
    "n_embd = 384\n",
    "n_head = 6\n",
    "n_layer = 6\n",
    "dropout = 0.2\n",
    "\n",
    "#model = BigramLanguageModel()\n",
    "model = GPTLanguageModel()\n",
    "m = model.to(device)\n",
    "# print the number of parameters in the model\n",
    "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
    "\n",
    "# create a PyTorch optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "o0ksqe6k4Pvp",
    "outputId": "311c6e1f-db56-4bbf-cdf1-7889eedb1100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "čas 0:01:00.15  271 z 5000 iterácií\n",
      "čas 0:02:00.34  542 z 5000 iterácií\n",
      "čas 0:03:00.47  807 z 5000 iterácií\n",
      "čas 0:04:00.50  1071 z 5000 iterácií\n",
      "čas 0:05:00.57  1336 z 5000 iterácií\n",
      "čas 0:06:00.73  1559 z 5000 iterácií\n",
      "čas 0:07:00.80  1724 z 5000 iterácií\n",
      "čas 0:08:01.17  1890 z 5000 iterácií\n",
      "čas 0:09:01.50  2055 z 5000 iterácií\n",
      "čas 0:10:01.55  2234 z 5000 iterácií\n",
      "čas 0:11:01.66  2504 z 5000 iterácií\n",
      "čas 0:12:01.83  2773 z 5000 iterácií\n",
      "čas 0:13:01.92  3039 z 5000 iterácií\n",
      "čas 0:14:02.07  3306 z 5000 iterácií\n",
      "čas 0:15:02.22  3563 z 5000 iterácií\n",
      "čas 0:16:02.28  3821 z 5000 iterácií\n",
      "čas 0:17:02.55  4055 z 5000 iterácií\n",
      "čas 0:18:02.68  4306 z 5000 iterácií\n",
      "čas 0:19:02.93  4535 z 5000 iterácií\n",
      "čas 0:20:02.97  4771 z 5000 iterácií\n",
      "čas 0:21:03.17  4996 z 5000 iterácií\n",
      "čas 0:21:03.97  4999 z 5000 iterácií\n",
      "Trénovanie neurónovej siete trvalo -21 minút\n"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "def format_td(seconds, digits=3):\n",
    "    isec, fsec = divmod(round(seconds*10**digits), 10**digits)\n",
    "    return f'{timedelta(seconds=isec)}.{fsec:0{digits}.0f}'\n",
    "\n",
    "#trénovanie pre daný počet iterácií\n",
    "\n",
    "start_time = time.time()\n",
    "n_time = time.time()\n",
    "for iter in range(max_iters):\n",
    "\n",
    "    # výpis aktuálnych hodnôt každú minútu\n",
    "    sec = (time.time()-start_time)\n",
    "    sec1 = (time.time()-n_time)\n",
    "    \n",
    "    if sec1 >= 60 or iter == max_iters - 1:  #každú minutu\n",
    "        #losses = estimate_loss()       \n",
    "        print(\"čas {} \".format(format_td(sec, digits=2)),\n",
    "               iter,\"z\",max_iters, \"iterácií\", \n",
    "              \n",
    "              )      \n",
    "        n_time = time.time()\n",
    "\n",
    "    # načítasnie dávky údajov z náhodného miesta textu\n",
    "    xb, yb = nacitanie_davky('train')\n",
    "\n",
    "    # evaluate the loss\n",
    "    logits, loss = model(xb, yb)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "print(\"Trénovanie neurónovej siete trvalo {} minút\".format(round((start_time - time.time())/60),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "uNMnOoZvmYIL",
    "outputId": "09d1c630-d220-44d6-957a-69be29a1aab0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " énevymienú Vlhkosť je plný rodičkou Semtasí priami leží Robota dolu len vyberie vodky po umývaní sa nad tiPustí na Rácza Rácza Teraz je to radiátorov prázdno hlavu nepriamo v tme a potom sa Vzadu je teda skoro rozň Drží rozhodne Ešte čoskoro ponite Khunt sa do nej dávno si obšmiel Ešte zo všetko robotu záprah Kotolník so zatvorenými očami Brúrkami od zimy Tváre mu však každý pregignál Ten vám neskle mal Venu že hyste tak ľutovať okradol prikáže To je to aby ste si on Hurensson ulaÁšte nie povie \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# generovanie textu\n",
    "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
    "print(cisla_na_text(m.generate(context, max_new_tokens=500)[0].tolist()))\n",
    "\n",
    "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
